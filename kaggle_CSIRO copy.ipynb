{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0900041",
   "metadata": {},
   "source": [
    "# CSIRO Biomass (Kaggle) — Baseline + Ablations\n",
    "\n",
    "## Baseline\n",
    "\n",
    "### Data & evaluation\n",
    "\n",
    "* **Tiny data (~357 unique train images)** → overfit risk is the main constraint.\n",
    "* **Train table is long-format:** ~1785 rows = 357 images × 5 targets → pivot to **wide (357 rows)** for training.\n",
    "* **Validation:** grouped CV by the true sampling unit (no row-level splits).\n",
    "\n",
    "  * candidate splits to try (ablation):\n",
    "\n",
    "    * **Group by `Sampling_Date`** (often improves CV↔LB realism)\n",
    "    * stratify by **State** (target distributions differ by state)\n",
    "* **Submission/inference:** images only (treat metadata as train-only).\n",
    "\n",
    "### Model\n",
    "\n",
    "* **Backbone:** DINOv3 (**frozen**)\n",
    "* **Neck:** none (baseline)\n",
    "\n",
    "### Head\n",
    "\n",
    "* **2-layer MLP** → **5 outputs** (`Green, Clover, Dead, GDM, Total`)\n",
    "* Use LayerNorm (or keep backbone norm), dropout ~0.1–0.3.\n",
    "\n",
    "### Targets & loss\n",
    "\n",
    "* Train on **log1p** targets.\n",
    "* Baseline loss: **weighted MSE** in log-space.\n",
    "\n",
    "  * weights roughly: `Green=0.1, Clover=0.1, Dead=0.1, GDM=0.2, Total=0.5`\n",
    "* Metric: global weighted R² (in original space) computed across all targets.\n",
    "\n",
    "### Augmentation (label-safe)\n",
    "\n",
    "Goal: improve robustness without breaking the “grams ↔ pixels” relationship.\n",
    "\n",
    "* Safe geometric:\n",
    "\n",
    "  * flips\n",
    "  * 90° rotations\n",
    "  * small translate/shear (no scale)\n",
    "  * **jigsaw (patch shuffle):** split into a small grid (e.g., 3×3) and randomly permute patches (use low probability; preserves scale but breaks global layout)\n",
    "* Photometric:\n",
    "\n",
    "  * brightness/contrast/saturation/hue jitter\n",
    "  * mild blur\n",
    "  * mild autocontrast\n",
    "    Avoid:\n",
    "* random resized crop / heavy zoom\n",
    "* cutout / random erasing\n",
    "* heavy rotations that require re-scaling\n",
    "\n",
    "### Training hygiene\n",
    "\n",
    "* Use AMP (BF16).\n",
    "* Consider cosine LR + warmup.\n",
    "* Consider EMA/SWA once baseline is stable.\n",
    "* Clip gradients (e.g., 1.0 norm) if training is noisy.\n",
    "\n",
    "## Ablations\n",
    "\n",
    "### A) Must-verify (rules + correctness)\n",
    "\n",
    "* Verify that each image has all 5 targets after pivot (wide).\n",
    "* Verify metric implementation matches the competition definition.\n",
    "* Verify no leakage: same `Sampling_Date` never appears in both train and val.\n",
    "\n",
    "### B) Baseline implementation decisions\n",
    "\n",
    "* Compare CV split strategies:\n",
    "\n",
    "  * GroupKFold by `Sampling_Date`\n",
    "  * StratifiedGroupKFold by `State` with groups=`Sampling_Date`\n",
    "* Compare head sizes / dropout.\n",
    "\n",
    "### C) CV evaluation protocol\n",
    "\n",
    "* Report mean ± std across folds.\n",
    "* Track per-target metrics for diagnostics.\n",
    "* Keep the split fixed across experiments.\n",
    "\n",
    "### D) Inference stability\n",
    "\n",
    "* Test-time augmentation (TTA): rotations/flips.\n",
    "* Average predictions in log-space vs original space.\n",
    "\n",
    "$1$2- Try **jigsaw (patch shuffle)**: 3×3 or 4×4 grid, low probability, validate under CV.\n",
    "\n",
    "$3 Tiling (multi-crop / multi-instance)\n",
    "\n",
    "* **Tiled backbone features:** split each image into an `n×n` grid (start with **2×2**) and run the frozen DINO backbone on each tile.\n",
    "* **Pooling (keep it simple):**\n",
    "\n",
    "  * mean pool tile embeddings → single feature → head\n",
    "  * (optional) mean+max concat (still simple)\n",
    "  * (optional) per-tile head then mean of predictions\n",
    "* **If images are stitched left/right:** tile each half separately → pool L and R → concat `[L, R]` → head.\n",
    "* **Ablate:** `n=1` (no tiling) vs `n=2` vs `n=3` (compute-heavy) and pooling choice.\n",
    "\n",
    "### G) Improvements (after baseline is stable)\n",
    "\n",
    "* Unfreeze last N blocks (careful with overfit).\n",
    "* SWA/EMA.\n",
    "* Better heads (depth/width).\n",
    "* Regularization sweeps.\n",
    "\n",
    "## Insights\n",
    "\n",
    "* Host: public/private split is **not fully random**; test includes some **non-overlapping** time/location periods.\n",
    "* Group by `Sampling_Date` to reduce leakage from date-correlated collection conditions.\n",
    "* With ~300 images, CV is noisy; report mean±std and avoid “seed shopping”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219733c8",
   "metadata": {},
   "source": [
    "**Repo note:** core code is also packaged under `src/csiro_biomass/` (see `scripts/train_cv.py`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2168e",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca52cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pathlib, yaml\n",
    "yaml_path = \"/notebooks/env.yaml\"\n",
    "\n",
    "with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "    env = yaml.safe_load(f)\n",
    "\n",
    "for k, v in env.items():\n",
    "    os.environ[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967469fd",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3caad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WB = \"https://dinov3.llamameta.net/dinov3_vitb16/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidW84aXJvdGQyeThwcGpuNXFveGthZTE4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjU5NzI4MTd9fX1dfQ__&Signature=H5H5kLVc6V83i-s2euNHx6t9KlVeG27QKX6qtkXNiLwEzuCshJD4RfwUbQv8oBJOZXPezAVJZPRkYRdsb4jh-LQ72DZtEuNkjNKHf7Pn57wzee0bjEYjWdJmOqK4waaSe9TQqELM%7EPgzdAT4LCSHYcFQ%7EleRnHGGGJiHBmTd6e1xZYhvUCfkvVD1TG-zM7R0-P%7EMLetHMvWl%7EUapCMYthsWqZctsYAQKUQxsLrly8Y4EaM8hm5nowpArPZC4myNO1iiXld5Hc3t9CVLEdYT7LIct0x6cf3-B-6WOgxGb7LdLPCcZPPfoGgX3KGtTAgNQYOpGFs-hgILFHRKVOJ7T3A__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1893388161261111\"\n",
    "WL = \"https://dinov3.llamameta.net/dinov3_vitl16/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidW84aXJvdGQyeThwcGpuNXFveGthZTE4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjU5NzI4MTd9fX1dfQ__&Signature=H5H5kLVc6V83i-s2euNHx6t9KlVeG27QKX6qtkXNiLwEzuCshJD4RfwUbQv8oBJOZXPezAVJZPRkYRdsb4jh-LQ72DZtEuNkjNKHf7Pn57wzee0bjEYjWdJmOqK4waaSe9TQqELM%7EPgzdAT4LCSHYcFQ%7EleRnHGGGJiHBmTd6e1xZYhvUCfkvVD1TG-zM7R0-P%7EMLetHMvWl%7EUapCMYthsWqZctsYAQKUQxsLrly8Y4EaM8hm5nowpArPZC4myNO1iiXld5Hc3t9CVLEdYT7LIct0x6cf3-B-6WOgxGb7LdLPCcZPPfoGgX3KGtTAgNQYOpGFs-hgILFHRKVOJ7T3A__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1893388161261111\"\n",
    "WL_plus = \"https://dinov3.llamameta.net/dinov3_vith16plus/dinov3_vith16plus_pretrain_lvd1689m-7c1da9a5.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidW84aXJvdGQyeThwcGpuNXFveGthZTE4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjU5NzI4MTd9fX1dfQ__&Signature=H5H5kLVc6V83i-s2euNHx6t9KlVeG27QKX6qtkXNiLwEzuCshJD4RfwUbQv8oBJOZXPezAVJZPRkYRdsb4jh-LQ72DZtEuNkjNKHf7Pn57wzee0bjEYjWdJmOqK4waaSe9TQqELM%7EPgzdAT4LCSHYcFQ%7EleRnHGGGJiHBmTd6e1xZYhvUCfkvVD1TG-zM7R0-P%7EMLetHMvWl%7EUapCMYthsWqZctsYAQKUQxsLrly8Y4EaM8hm5nowpArPZC4myNO1iiXld5Hc3t9CVLEdYT7LIct0x6cf3-B-6WOgxGb7LdLPCcZPPfoGgX3KGtTAgNQYOpGFs-hgILFHRKVOJ7T3A__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1893388161261111\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbbdf3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"file:///notebooks/dinov3/weights/dinov3_vitb16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov3_vitb16_pretrain.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327M/327M [00:01<00:00, 183MB/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import uuid\n",
    "\n",
    "\n",
    "\"\"\"os.environ[\"TORCHINDUCTOR_FX_GRAPH_CACHE\"]=\"1\"\n",
    "os.environ[\"TORCHINDUCTOR_AUTOGRAD_CACHE\"]=\"1\"\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = \"/notebooks/dinov3/compile_cache\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import comet_ml\n",
    "\n",
    "\n",
    "sys.path.insert(0, \"/notebooks/dinov3\")  # your fork\n",
    "from dinov3.layers.block import SelfAttentionBlock\n",
    "\n",
    "\"\"\"torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(False)\n",
    "\n",
    "dynamo_config = torch._dynamo.config\n",
    "dynamo_config.compiled_autograd = True\n",
    "dynamo_config.capture_scalar_outputs = False\n",
    "dynamo_config.cache_size_limit = 512\"\"\"\n",
    "\n",
    "#torch.set_float32_matmul_precision(\"highest\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# data: long -> wide\n",
    "# -------------------------\n",
    "def load_train_wide(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    idx_cols = [\"image_path\", \"Sampling_Date\", \"State\", \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"]\n",
    "    wide = (\n",
    "        df.pivot_table(index=idx_cols, columns=\"target_name\", values=\"target\", aggfunc=\"first\")\n",
    "          .reset_index()\n",
    "    )\n",
    "    for t in TARGETS:\n",
    "        if t not in wide.columns:\n",
    "            wide[t] = np.nan\n",
    "    wide = wide.dropna(subset=TARGETS).reset_index(drop=True)\n",
    "    wide[\"abs_path\"] = wide[\"image_path\"].apply(lambda p: os.path.join(ROOT, p))\n",
    "    return wide\n",
    "\n",
    "model_size = \"b\"\n",
    "W = WB\n",
    "plus = \"\"\n",
    "COMPILE_MODEL = False\n",
    "REPO_DIR = \"/notebooks/dinov3\"\n",
    "DINO_WEIGHTS = f\"/notebooks/dinov3/weights/dinov3_vit{model_size}16_pretrain{plus}.pth\"\n",
    "MODEL = torch.hub.load(REPO_DIR, f'dinov3_vit{model_size}16{plus.replace(\"_\", \"\")}', source='local', weights=DINO_WEIGHTS, verbose=True)\n",
    "#MODEL_PLUS = torch.hub.load(REPO_DIR, f'dinov3_vit{model_size}16plus', source='local', weights=WL_plus, verbose=True)\n",
    "NUM_WORKERS = os.cpu_count() - 2\n",
    "ROOT = \"/notebooks/kaggle/csiro\"\n",
    "CSV_PATH = os.path.join(ROOT, \"train.csv\")\n",
    "TARGETS = [\"Dry_Green_g\", \"Dry_Clover_g\", \"Dry_Dead_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "WIDE_DF=load_train_wide(CSV_PATH)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "IMG_SIZE = 512\n",
    "SEED = 420\n",
    "DTYPE = torch.bfloat16  # set to torch.bfloat16 on GPUs that support it\n",
    "RUN_SWEEPS = True  # set True to run CV sweeps\n",
    "FEAT_DIM = MODEL.norm.normalized_shape[0]\n",
    "NUM_HEADS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2d1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, out_path: str, chunk_size: int = 1024 * 1024):\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:  # filter keep-alive chunks\n",
    "                    f.write(chunk)\n",
    "    return out_path\n",
    "\n",
    "# example\n",
    "url = \"https://dinov3.llamameta.net/dinov3_vitl16/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidW84aXJvdGQyeThwcGpuNXFveGthZTE4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjU5NzI4MTd9fX1dfQ__&Signature=H5H5kLVc6V83i-s2euNHx6t9KlVeG27QKX6qtkXNiLwEzuCshJD4RfwUbQv8oBJOZXPezAVJZPRkYRdsb4jh-LQ72DZtEuNkjNKHf7Pn57wzee0bjEYjWdJmOqK4waaSe9TQqELM%7EPgzdAT4LCSHYcFQ%7EleRnHGGGJiHBmTd6e1xZYhvUCfkvVD1TG-zM7R0-P%7EMLetHMvWl%7EUapCMYthsWqZctsYAQKUQxsLrly8Y4EaM8hm5nowpArPZC4myNO1iiXld5Hc3t9CVLEdYT7LIct0x6cf3-B-6WOgxGb7LdLPCcZPPfoGgX3KGtTAgNQYOpGFs-hgILFHRKVOJ7T3A__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1893388161261111\"\n",
    "out_path = \"/notebooks/dinov3/weights/dinov3_vitl16_pretrain.pth\"\n",
    "#download_file(url, out_path)\n",
    "#print(\"Saved to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b93b45",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5eb40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _denorm_img(x: torch.Tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: [3,H,W] float tensor normalized with mean/std.\n",
    "    returns: [3,H,W] in [0,1]\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(mean, dtype=x.dtype, device=x.device).view(3, 1, 1)\n",
    "    std  = torch.tensor(std,  dtype=x.dtype, device=x.device).view(3, 1, 1)\n",
    "    x = x * std + mean\n",
    "    return x.clamp(0, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def show_nxn_grid(dataset=None, dataloader=None, n=4, indices=None, seed=0,\n",
    "                  mean=IMAGENET_MEAN, std=IMAGENET_STD,\n",
    "                  show_targets=True, targets_are_log1p=True, figsize_per_cell=3.0):\n",
    "    assert (dataset is not None) ^ (dataloader is not None), \"Pass exactly one of dataset or dataloader.\"\n",
    "    k = n * n\n",
    "    xs, ys = [], []\n",
    "    if dataset is not None:\n",
    "        if indices is None:\n",
    "            rng = random.Random(seed)\n",
    "            indices = [rng.randrange(len(dataset)) for _ in range(k)]\n",
    "        else:\n",
    "            assert len(indices) >= k, f\"Need at least {k} indices.\"\n",
    "\n",
    "        for i in indices[:k]:\n",
    "            x, y = dataset[i]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "        x_batch = torch.stack(xs, dim=0)  # [k,3,H,W]\n",
    "        y_batch = torch.stack(ys, dim=0) if show_targets else None\n",
    "\n",
    "    else:\n",
    "        for xb, yb in dataloader:\n",
    "            for j in range(xb.shape[0]):\n",
    "                xs.append(xb[j])\n",
    "                ys.append(yb[j])\n",
    "                if len(xs) >= k:\n",
    "                    break\n",
    "            if len(xs) >= k:\n",
    "                break\n",
    "\n",
    "        x_batch = torch.stack(xs, dim=0)\n",
    "        y_batch = torch.stack(ys, dim=0) if show_targets else None\n",
    "\n",
    "    # plot\n",
    "    fig, axes = plt.subplots(n, n, figsize=(n * figsize_per_cell, n * figsize_per_cell))\n",
    "    axes = np.asarray(axes)\n",
    "\n",
    "    for idx in range(k):\n",
    "        ax = axes[idx // n, idx % n]\n",
    "        x = _denorm_img(x_batch[idx], mean=mean, std=std)\n",
    "        img = x.permute(1, 2, 0).cpu().numpy()  # [H,W,3] in [0,1]\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        if show_targets and (y_batch is not None):\n",
    "            y = y_batch[idx].detach().cpu()\n",
    "            if targets_are_log1p:\n",
    "                y = torch.expm1(y).clamp_min(0.0)\n",
    "            # short title\n",
    "            ax.set_title(\" \".join([f\"{v:.2f}\" for v in y.tolist()]), fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class TileEncoder(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, input_res: int):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.input_res = input_res\n",
    "\n",
    "    def forward(self, x: torch.Tensor, grid):\n",
    "        B, C, H, W = x.shape\n",
    "        r, c = grid\n",
    "        hs = torch.linspace(0, H, steps=r + 1, device=x.device).round().long()\n",
    "        ws = torch.linspace(0, W, steps=c + 1, device=x.device).round().long()\n",
    "        tiles = []\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                rs, re = hs[i].item(), hs[i + 1].item()\n",
    "                cs, ce = ws[j].item(), ws[j + 1].item()\n",
    "                xt = x[:, :, rs:re, cs:ce]\n",
    "                if xt.shape[-2:] != (self.input_res, self.input_res):\n",
    "                    xt = F.interpolate(xt, size=(self.input_res, self.input_res), mode=\"bilinear\", align_corners=False)\n",
    "                tiles.append(xt)\n",
    "        tiles = torch.stack(tiles, dim=1)\n",
    "        flat = tiles.view(-1, C, self.input_res, self.input_res)\n",
    "        feats = self.backbone(flat)\n",
    "        feats = feats.view(B, -1, feats.shape[-1])\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36b7ff",
   "metadata": {},
   "source": [
    "# Train Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc093015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# transforms\n",
    "# -------------------------\n",
    "class PadToSquare:\n",
    "    def __init__(self, fill=0):\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        if w == h:\n",
    "            return img\n",
    "        s = max(w, h)\n",
    "        new = Image.new(img.mode, (s, s), color=self.fill)\n",
    "        new.paste(img, ((s - w) // 2, (s - h) // 2))\n",
    "        return new\n",
    "\n",
    "def get_tfms():\n",
    "    return T.Compose([\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomVerticalFlip(p=0.5),\n",
    "        T.RandomChoice([\n",
    "            T.Lambda(lambda x: x),\n",
    "            T.RandomRotation((90, 90)),\n",
    "            T.RandomRotation((180, 180)),\n",
    "            T.RandomRotation((270, 270)),\n",
    "        ]),\n",
    "        T.ColorJitter(brightness=0.20, contrast=0.20, saturation=0.20, hue=0.04),\n",
    "\n",
    "    ])\n",
    "\n",
    "def post_tfms(): \n",
    "    normalize = T.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                            std=(0.229, 0.224, 0.225))\n",
    "\n",
    "    return T.Compose([T.ToTensor(),normalize])\n",
    "# -------------------------\n",
    "# dataset\n",
    "# -------------------------\n",
    "\n",
    "class BiomassBaseCached(Dataset):\n",
    "    \"\"\"Caches resized/padded PIL images + stores y_log once.\"\"\"\n",
    "    def __init__(self, wide_df, img_size=IMG_SIZE):\n",
    "        self.df = wide_df.reset_index(drop=True)\n",
    "        y = self.df[TARGETS].values.astype(np.float32)\n",
    "        self.y_log = np.log1p(y)\n",
    "\n",
    "        # cache at fixed size (PIL)\n",
    "        pre = T.Compose([\n",
    "            PadToSquare(fill=0),\n",
    "            T.Resize((img_size, img_size), antialias=True),\n",
    "        ])\n",
    "        self.imgs = []\n",
    "        for p in self.df[\"abs_path\"].tolist():\n",
    "            im = Image.open(p).convert(\"RGB\")\n",
    "            im = pre(im)\n",
    "            self.imgs.append(im.copy())\n",
    "            im.close()\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.imgs[i], torch.from_numpy(self.y_log[i])  # PIL, y_log\n",
    "\n",
    "\n",
    "class TransformView(Dataset):\n",
    "    \"\"\"Applies train/val transforms on top of the same cached base dataset.\"\"\"\n",
    "    def __init__(self, base: BiomassBaseCached, tfms):\n",
    "        self.base = base\n",
    "        self.tfms = tfms\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img, y = self.base[i]          # img is cached PIL\n",
    "        x = self.tfms(img)             # apply aug+tensor+norm OR tensor+norm\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "# -------------------------\n",
    "# Loss\n",
    "# -------------------------  \n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, weights=(0.1, 0.1, 0.1, 0.2, 0.5), normalize=True):\n",
    "        super().__init__()\n",
    "        w = torch.as_tensor(weights, dtype=torch.float32)\n",
    "        self.register_buffer(\"w\", w)\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, pred_log: torch.Tensor, target_log: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.w.view(1, -1)\n",
    "        err2 = (pred_log - target_log).pow(2)\n",
    "        loss = (err2 * w).sum(dim=-1)\n",
    "        if self.normalize:\n",
    "            loss = loss / (self.w.sum() + 1e-12)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# model: frozen DINOv3 + head\n",
    "# -------------------------\n",
    "class DINOv3Regressor(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, hidden=1024, depth=2, drop=0.1, out_dim=5, feat_dim = None, norm=None, num_neck=1):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        feat_dim = feat_dim or FEAT_DIM\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        self.backbone.eval()\n",
    "\n",
    "        assert not num_neck or feat_dim == 768, \"Only VIT B is supported for now for neck\"\n",
    "        neck = []\n",
    "        for _ in range(num_neck):\n",
    "            neck.append(SelfAttentionBlock(feat_dim, num_heads=12))\n",
    "        self.neck = nn.Sequential(*neck) if neck else []\n",
    "\n",
    "        if depth < 2:\n",
    "            raise ValueError(f\"depth must be >= 2 (got {depth})\")\n",
    "        \n",
    "        layers = []\n",
    "        in_dim = feat_dim\n",
    "        norm_layer = norm or nn.LayerNorm\n",
    "        for _ in range(depth - 1):\n",
    "            layers += [nn.Linear(in_dim, hidden), norm_layer(hidden), nn.GELU(), nn.Dropout(drop)]\n",
    "            in_dim = hidden\n",
    "        layers += [nn.Linear(in_dim, out_dim)]\n",
    "        self.head = nn.Sequential(*layers)\n",
    "        \n",
    "        self.norm = norm_layer(feat_dim)\n",
    "        self.init()\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x, rope = self.backbone(x)\n",
    "            x = x[\"x_prenorm\"]\n",
    "            \n",
    "        for neck in self.neck:\n",
    "            x = neck(x, rope)\n",
    "        x = self.norm(x[: , 0, :])\n",
    "        return self.head(x)  \n",
    "    \n",
    "    def set_train(self, train = True):\n",
    "        self.neck.train(train)\n",
    "        self.head.train(train)\n",
    "        self.norm.train(train)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def init(self):\n",
    "        modules = [*self.head.modules(), *self.neck.modules(), *self.norm.modules()]\n",
    "        for m in modules:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                if m.elementwise_affine:\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d27c93",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94945377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_global_wr2(model, dl_va, w_vec, device=\"cuda\"):\n",
    "    model.set_train(False)\n",
    "    w5 = w_vec.to(device).view(1, -1)\n",
    "    ss_res  = torch.zeros((), device=device)\n",
    "    sum_w   = torch.zeros((), device=device)\n",
    "    sum_wy  = torch.zeros((), device=device)\n",
    "    sum_wy2 = torch.zeros((), device=device)\n",
    "\n",
    "    with torch.inference_mode(), torch.amp.autocast(device_type=\"cuda\", dtype=DTYPE, enabled=device.startswith(\"cuda\")):\n",
    "        for x, y_log in dl_va:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y_log = y_log.to(device, non_blocking=True)   # log1p targets\n",
    "            p_log = model(x).float()                      # log1p preds\n",
    "\n",
    "            y = torch.expm1(y_log)\n",
    "            p = torch.expm1(p_log).clamp_min(0.0)\n",
    "\n",
    "            w = w5.expand_as(y)                           # [B, 5]\n",
    "            diff = (y - p)\n",
    "\n",
    "            ss_res  += (w * diff * diff).sum()\n",
    "            sum_w   += w.sum()\n",
    "            sum_wy  += (w * y).sum()\n",
    "            sum_wy2 += (w * y * y).sum()\n",
    "\n",
    "    mu = sum_wy / (sum_w + 1e-12)\n",
    "    ss_tot = sum_wy2 - sum_w * mu * mu\n",
    "    score = (1.0 - ss_res / (ss_tot + 1e-12)).item()\n",
    "    return score\n",
    "\n",
    "def cos_sin_lr(ep: int, epochs: int, lr_start: float, lr_final: float) -> float:\n",
    "    if epochs <= 1:\n",
    "        return lr_final\n",
    "    t = (ep - 1) / (epochs - 1)  # 0 -> 1\n",
    "    return lr_final + 0.5 * (lr_start - lr_final) * (1.0 + math.cos(math.pi * t))\n",
    "\n",
    "def set_optimizer_lr(opt, lr: float):\n",
    "    for pg in opt.param_groups:\n",
    "        pg[\"lr\"] = lr\n",
    "\n",
    "def train_one_fold(\n",
    "    ds_tr_view,\n",
    "    ds_va_view,\n",
    "    backbone,\n",
    "    tr_idx,\n",
    "    va_idx,\n",
    "    wd=1e-4,\n",
    "    fold_idx=0,\n",
    "    epochs=5,\n",
    "    lr_start=3e-4,\n",
    "    lr_final=5e-5,\n",
    "    batch_size=128,\n",
    "    device=\"cuda\",\n",
    "    save_path=None,\n",
    "    verbose=False,\n",
    "    plot_imgs = False,\n",
    "    early_stopping=6,\n",
    "    head_hidden = 1024,\n",
    "    head_depth = 2,\n",
    "    head_drop = 0.1,\n",
    "    num_neck = 0,\n",
    "    comet_exp = None,\n",
    "    skip_log_first_n = 5,\n",
    "    curr_fold = 0\n",
    "):\n",
    "    tr_subset = Subset(ds_tr_view, tr_idx)\n",
    "    va_subset = Subset(ds_va_view, va_idx)\n",
    "\n",
    "    dl_kwargs = dict(\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        persistent_workers=(NUM_WORKERS > 0),\n",
    "    )\n",
    "    dl_tr = DataLoader(tr_subset, shuffle=True, **dl_kwargs)\n",
    "    dl_va = DataLoader(va_subset, shuffle=False, **dl_kwargs)\n",
    "    \n",
    "    if plot_imgs:\n",
    "        show_nxn_grid(dataloader=dl_tr, n=4)\n",
    "        return \n",
    "    \n",
    "    if comet_exp is not None:\n",
    "        for k, v in locals().items():\n",
    "            if isinstance(v, (int, float, str)):\n",
    "                comet_exp.log_parameter(k, v)\n",
    "\n",
    "    model = DINOv3Regressor(\n",
    "        backbone, hidden=head_hidden, drop=head_drop, depth=head_depth, num_neck=num_neck\n",
    "        ).to(device)\n",
    "    model.init()\n",
    "    criterion = WeightedMSELoss().to(device) \n",
    "    opt = torch.optim.AdamW(model.head.parameters(), lr=lr_start, weight_decay=wd)\n",
    "    scaler = None\n",
    "    if device.startswith('cuda') and DTYPE == torch.float16:\n",
    "        scaler = torch.amp.GradScaler()\n",
    "\n",
    "    best_score = -1e9\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "    p_bar = tqdm(range(1, epochs + 1))\n",
    "    for ep in p_bar:\n",
    "        lr = cos_sin_lr(ep, epochs, lr_start, lr_final)\n",
    "        set_optimizer_lr(opt, lr)\n",
    "        model.set_train(True)\n",
    "        running = 0.0\n",
    "        n_seen = 0\n",
    "\n",
    "        for x, y_log in dl_tr:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y_log = y_log.to(device, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=DTYPE, enabled=device.startswith(\"cuda\")):\n",
    "                p_log = model(x)\n",
    "                loss = criterion(p_log, y_log)\n",
    "\n",
    "                if scaler is not None:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "\n",
    "            bs = x.size(0)\n",
    "            running += loss.detach() * bs\n",
    "            n_seen += bs\n",
    "\n",
    "        train_loss = (running / max(n_seen, 1)).item()\n",
    "        score = eval_global_wr2(model, dl_va, criterion.w, device=device)\n",
    "        if comet_exp is not None and ep > skip_log_first_n:\n",
    "            to_log = {f\"train_loss_{curr_fold}\": train_loss, f\"val_wR2_{curr_fold}\": score}\n",
    "            comet_exp.log_metrics(to_log, step=ep)\n",
    "        \n",
    "        if save_path and score > best_score:\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.head.state_dict().items()}\n",
    "            \n",
    "        if score > best_score:\n",
    "            patience = 0\n",
    "            best_score = score\n",
    "        else:\n",
    "            patience += 1\n",
    "        \n",
    "        s1 = f\"Best score: {best_score:.4f} | Patience: {patience:02d}/{early_stopping:02d} | lr: {lr:6.4f}\"\n",
    "        s2 = f\"[fold {fold_idx}] | train_loss={train_loss:.4f} | val_wR2={score:.4f} | {s1}\"\n",
    "        if verbose:\n",
    "            print(s2)\n",
    "            \n",
    "        if patience >= early_stopping:\n",
    "            if verbose:\n",
    "                print(\"Early stopping...\")\n",
    "            p_bar.set_postfix_str(s2 + \" | Early stopping...\")\n",
    "            break\n",
    "            \n",
    "        p_bar.set_postfix_str(s2)\n",
    "    p_bar.close()\n",
    "\n",
    "    if best_state is not None:\n",
    "        torch.save(best_state, save_path)\n",
    "\n",
    "    return best_score\n",
    "\n",
    "\n",
    "def run_groupkfold_cv(\n",
    "    dataset,\n",
    "    wide_df,\n",
    "    n_splits=5,\n",
    "    group_col=\"Sampling_Date\",\n",
    "    tfms_fn = get_tfms,\n",
    "    comet_exp_name = None,\n",
    "    sweep_config = \"\",\n",
    "    **train_kwargs,\n",
    "):\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    X = wide_df  \n",
    "    y = wide_df[\"State\"].values             \n",
    "    groups = wide_df[group_col].values\n",
    "    ds_tr_view = TransformView(dataset, T.Compose([tfms_fn(), post_tfms()]))\n",
    "    ds_va_view = TransformView(dataset, post_tfms())\n",
    "    \n",
    "    \"\"\"\n",
    "    bb_copy = copy.deepcopy(backbone)\n",
    "    model = DINOv3Regressor(bb_copy, hidden=head_hidden, drop=head_drop, depth=head_depth, norm=head_norm).to(device)\n",
    "    \n",
    "    if COMPILE_MODEL:\n",
    "        model.compile(fullgraph=True, mode=\"default\", backend=\"inductor\", dynamic=True)\n",
    "    \"\"\"\n",
    "    if comet_exp_name is not None:\n",
    "        comet_exp = comet_ml.start(\n",
    "            api_key=os.getenv(\"COMET_API_KEY\"),\n",
    "            project_name=comet_exp_name,\n",
    "            experiment_key=None\n",
    "        )\n",
    "    fold_scores = []\n",
    "    try:\n",
    "        comet_exp.set_name(comet_exp_name + \"_\" + sweep_config + \"_\" + str(uuid.uuid4())[:3])\n",
    "        for fold_idx, (tr_idx, va_idx) in enumerate(sgkf.split(X, y, groups)):\n",
    "                score = train_one_fold(\n",
    "                    ds_tr_view=ds_tr_view,\n",
    "                    ds_va_view=ds_va_view,\n",
    "                    tr_idx=tr_idx,\n",
    "                    va_idx=va_idx,\n",
    "                    fold_idx=fold_idx,\n",
    "                    device=\"cuda\",\n",
    "                    comet_exp = comet_exp,\n",
    "                    curr_fold = fold_idx,\n",
    "                    **train_kwargs,\n",
    "                )\n",
    "                if \"plot_imgs\" in train_kwargs and train_kwargs[\"plot_imgs\"]:\n",
    "                    return None, None, None\n",
    "                fold_scores.append(score)\n",
    "    except Exception as e:\n",
    "        print(f\"Fold {fold_idx} failed with exception: {e}\")\n",
    "    finally:\n",
    "        if comet_exp is not None:\n",
    "            comet_exp.end()\n",
    "\n",
    "    fold_scores = np.array(fold_scores, dtype=np.float32)\n",
    "    mean = float(fold_scores.mean())\n",
    "    std = float(fold_scores.std(ddof=0))\n",
    "\n",
    "    print(\"\\nCV summary\")\n",
    "    for i, s in enumerate(fold_scores.tolist()):\n",
    "        print(f\"  fold {i}: {s:.4f}\")\n",
    "    print(f\"  mean ± std: {mean:.4f} ± {std:.4f}\")\n",
    "    return fold_scores, mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f84208",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_biomass = BiomassBaseCached(WIDE_DF, img_size=IMG_SIZE)\n",
    "assert RUN_SWEEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3710d",
   "metadata": {},
   "source": [
    "# Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b247eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfms_0():\n",
    "    return T.Compose([\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomVerticalFlip(p=0.5),\n",
    "        T.RandomChoice([\n",
    "            T.Lambda(lambda x: x),\n",
    "            T.RandomRotation((90, 90)),\n",
    "            T.RandomRotation((180, 180)),\n",
    "            T.RandomRotation((270, 270)),\n",
    "        ]),\n",
    "        T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.04),\n",
    "\n",
    "    ])\n",
    "\n",
    "def get_tfms_1():\n",
    "    return T.Compose([\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomVerticalFlip(p=0.5),\n",
    "        T.RandomChoice([\n",
    "            T.Lambda(lambda x: x),\n",
    "            T.RandomRotation((90, 90)),\n",
    "            T.RandomRotation((180, 180)),\n",
    "            T.RandomRotation((270, 270)),\n",
    "        ]),\n",
    "        T.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.04),\n",
    "\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd8080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/v1kstrand/cv5-51da/d6b582f6e8a54d9e805520c976713f63\n",
      "\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Error logging git-related information\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da1b2c698084449a2c32c7b2defa6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_kwargs = dict(\n",
    "    dataset=dataset_biomass,\n",
    "    wide_df=WIDE_DF,\n",
    "    backbone=MODEL,\n",
    "    epochs=80,\n",
    "    batch_size=64,\n",
    "    wd=3e-3,\n",
    "    head_hidden=2048,\n",
    "    head_drop=0.1,\n",
    "    head_depth=5,\n",
    "    plot_imgs=False,\n",
    "    early_stopping=15,\n",
    "    comet_exp_name=\"cv5-51da\",\n",
    ")\n",
    "\n",
    "sweeps = [\n",
    "    dict(num_neck=1, head_depth=4, tfms_fn=get_tfms_0),\n",
    "    dict(num_neck=1, head_depth=4, tfms_fn=get_tfms_1),\n",
    "\n",
    "]\n",
    "\n",
    "sweep_id = str(uuid.uuid4())[:4]\n",
    "for sweep in sweeps: \n",
    "    new_train_kwargs = train_kwargs.copy() \n",
    "    for k, v in sweep.items(): \n",
    "        new_train_kwargs[k] = v\n",
    "    #new_train_kwargs[\"comet_exp_name\"] += f\"-{sweep_id}\"\n",
    "    new_train_kwargs[\"sweep_config\"] = str(sweep)\n",
    "    scores, mean, std = run_groupkfold_cv(**new_train_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24edd278",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.7.1 (cu118)",
   "language": "python",
   "name": "pt27cu118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
