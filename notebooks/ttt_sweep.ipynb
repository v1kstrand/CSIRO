{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2ae06b90",
      "metadata": {},
      "source": [
        "# TTT Sweep: Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "68e40d79",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "CSIRO_CODE_DIR = \"/notebooks/CSIRO\"\n",
        "sys.path.insert(0, CSIRO_CODE_DIR)\n",
        "os.environ[\"DEFAULT_DINO_REPO_DIR\"]=\"/notebooks/dinov3\"\n",
        "os.environ[\"DEFAULT_DATA_ROOT\"]=\"/notebooks/kaggle/csiro\"\n",
        "os.environ[\"DINO_WEIGHTS_PATH\"] = \"/notebooks/kaggle/csiro/weights/dinov3/dinov3_vitb16_pretrain.pth\"\n",
        "os.environ[\"DINO_B_WEIGHTS_PATH\"] = \"/notebooks/kaggle/csiro/weights/dinov3/dinov3_vitb16_pretrain.pth\"\n",
        "os.environ[\"DINO_L_WEIGHTS_PATH\"] = \"/notebooks/kaggle/csiro/weights/dinov3/dinov3_vitl16_pretrain.pth\"\n",
        "\n",
        "from csiro.config import DEFAULTS, DEFAULT_DATA_ROOT, DEFAULT_DINO_REPO_DIR, DINO_WEIGHTS_PATH, dino_hub_name\n",
        "from csiro.data import load_train_wide, BiomassTiledCached, TiledTransformView\n",
        "from csiro.eval import ttt_sweep_cv\n",
        "from csiro.transforms import post_tfms\n",
        "\n",
        "# --- paths / config ---\n",
        "TRAIN_CSV = \"/notebooks/kaggle/csiro/train.csv\"  # e.g. \"/notebooks/kaggle/csiro/train.csv\"\n",
        "DATA_ROOT = DEFAULT_DATA_ROOT\n",
        "DINO_REPO = DEFAULT_DINO_REPO_DIR\n",
        "DINO_WEIGHTS = DINO_WEIGHTS_PATH\n",
        "PT_PATHS = [\n",
        "     \"/notebooks/kaggle/csiro/output/v7_n_models2_f2c.pt\",\n",
        "]\n",
        "CV_PARAMS = dict(mode=\"gkf\", cv_seed=126015, n_splits=5)\n",
        "\n",
        "IMG_SIZE = int(DEFAULTS.get(\"img_size\", 512))\n",
        "CACHE_IMAGES = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d7f2de1",
      "metadata": {},
      "source": [
        "# TTT Sweep: Load Data & Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "96a1c2f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- data ---\n",
        "wide_df = load_train_wide(TRAIN_CSV, root=DATA_ROOT)\n",
        "base_ds = BiomassTiledCached(wide_df, img_size=IMG_SIZE, cache_images=CACHE_IMAGES)\n",
        "dataset = TiledTransformView(\n",
        "    base_ds,\n",
        "    post_tfms(),\n",
        "    tile_swap=False,\n",
        ")\n",
        "\n",
        "# --- backbone ---\n",
        "backbone = torch.hub.load(\n",
        "    DINO_REPO,\n",
        "    dino_hub_name(model_size=str(DEFAULTS.get(\"backbone_size\", \"b\")), plus=str(DEFAULTS.get(\"plus\", \"\"))),\n",
        "    source=\"local\",\n",
        "    weights=DINO_WEIGHTS,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85863f5a",
      "metadata": {},
      "source": [
        "# TTT Sweep: Define Tasks + Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d427da",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b756ff787c64dc4b0d2fa21bdc29e2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "TTT sweeps:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac0bf4c075bb4856929745a472f374bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "rdrop_head folds:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'name': 'rdrop_head',\n",
              "  'steps': 4,\n",
              "  'lr': 0.001,\n",
              "  'beta': 0.0,\n",
              "  'batch_size': 1,\n",
              "  'inner_agg': 'mean',\n",
              "  'outer_agg': 'mean',\n",
              "  'fold_base': [0.7638000249862671,\n",
              "   0.791303277015686,\n",
              "   0.8119305372238159,\n",
              "   0.7919535636901855,\n",
              "   0.6741432547569275],\n",
              "  'fold_ttt': [0.765116810798645,\n",
              "   0.7909208536148071,\n",
              "   0.8113319277763367,\n",
              "   0.7917848229408264,\n",
              "   0.6740978956222534],\n",
              "  'fold_delta': [0.0013167858123779297,\n",
              "   -0.00038242340087890625,\n",
              "   -0.000598609447479248,\n",
              "   -0.00016874074935913086,\n",
              "   -4.5359134674072266e-05],\n",
              "  'mean_base': 0.7666261315345764,\n",
              "  'mean_ttt': 0.7666504621505738,\n",
              "  'mean_delta': 2.4330615997314452e-05}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision.transforms.functional as TF\n",
        "from csiro.config import IMAGENET_MEAN, IMAGENET_STD\n",
        "\n",
        "class RDropPredMSE(torch.nn.Module):\n",
        "    # Example task: MSE between two dropout passes on final preds.\n",
        "    def __init__(self, name: str = \"rdrop_pred_mse\"):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, model, x, ctx):\n",
        "        if hasattr(model, \"set_train\"):\n",
        "            model.set_train(True)\n",
        "        model.train()\n",
        "        p1 = model(x)\n",
        "        p2 = model(x)\n",
        "        return ((p1.float() - p2.float()) ** 2).mean()\n",
        "\n",
        "\n",
        "class JitterRotPredMSE(torch.nn.Module):\n",
        "    # Color jitter + rot90 invariance on final preds.\n",
        "    def __init__(self, bcs_val: float = 0.2, hue_val: float = 0.02, rot_k: int = 1, name: str = \"jitter_rot_pred_mse\"):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.bcs_val = float(bcs_val)\n",
        "        self.hue_val = float(hue_val)\n",
        "        self.rot_k = int(rot_k)\n",
        "\n",
        "    def _apply_jitter_batch(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.bcs_val <= 0.0 and self.hue_val <= 0.0:\n",
        "            return x\n",
        "        if self.bcs_val > 0.0:\n",
        "            b = float(self.bcs_val)\n",
        "            brightness = float(1.0 + (torch.rand((), device=x.device) * 2.0 - 1.0) * b)\n",
        "            contrast = float(1.0 + (torch.rand((), device=x.device) * 2.0 - 1.0) * b)\n",
        "            saturation = float(1.0 + (torch.rand((), device=x.device) * 2.0 - 1.0) * b)\n",
        "            x = TF.adjust_brightness(x, brightness)\n",
        "            x = TF.adjust_contrast(x, contrast)\n",
        "            x = TF.adjust_saturation(x, saturation)\n",
        "        if self.hue_val > 0.0:\n",
        "            hue = float((torch.rand((), device=x.device) * 2.0 - 1.0) * self.hue_val)\n",
        "            x = TF.adjust_hue(x, hue)\n",
        "        return x\n",
        "\n",
        "    def forward(self, model, x, ctx):\n",
        "        if hasattr(model, \"set_train\"):\n",
        "            model.set_train(True)\n",
        "        model.train()\n",
        "        if x.ndim != 5 or x.size(1) != 2:\n",
        "            raise ValueError(f\"Expected tiled input [B,2,C,H,W], got {tuple(x.shape)}\")\n",
        "\n",
        "        x = x.float()\n",
        "        mean = torch.tensor(IMAGENET_MEAN, device=x.device).view(1, 1, 3, 1, 1)\n",
        "        std = torch.tensor(IMAGENET_STD, device=x.device).view(1, 1, 3, 1, 1)\n",
        "        x_unn = (x * std + mean).clamp(0.0, 1.0)\n",
        "\n",
        "        b, tiles, c, h, w = x_unn.shape\n",
        "        x_flat = x_unn.view(b * tiles, c, h, w)\n",
        "        x1 = self._apply_jitter_batch(x_flat)\n",
        "        x2 = self._apply_jitter_batch(x_flat)\n",
        "        x2 = torch.rot90(x2, k=self.rot_k, dims=(-2, -1))\n",
        "\n",
        "        mean_f = mean.view(1, 3, 1, 1)\n",
        "        std_f = std.view(1, 3, 1, 1)\n",
        "        x1 = (x1 - mean_f) / std_f\n",
        "        x2 = (x2 - mean_f) / std_f\n",
        "\n",
        "        x1 = x1.view(b, tiles, c, h, w)\n",
        "        x2 = x2.view(b, tiles, c, h, w)\n",
        "        p1 = model(x1)\n",
        "        p2 = model(x2)\n",
        "        return ((p1.float() - p2.float()) ** 2).mean()\n",
        "\n",
        "\n",
        "TASKS = [\n",
        "    #RDropPredMSE(),\n",
        "    JitterRotPredMSE(),\n",
        "]\n",
        "\n",
        "# Param specs: strings can be module names (\"head\", \"neck\", \"norm\") or exact parameter names.\n",
        "PARAM_SPECS = [\n",
        "    [\"head\"],\n",
        "]\n",
        "\n",
        "SWEEPS = [\n",
        "    dict(\n",
        "        name=\"rdrop_head\",\n",
        "        task=TASKS[0],\n",
        "        params=PARAM_SPECS[0],\n",
        "        steps=4,\n",
        "        lr=1e-4,\n",
        "        beta=0.0,\n",
        "        batch_size=1,\n",
        "    ),\n",
        "]\n",
        "\n",
        "results = ttt_sweep_cv(\n",
        "    dataset=dataset,\n",
        "    wide_df=wide_df,\n",
        "    backbone=backbone,\n",
        "    pt_paths=PT_PATHS,\n",
        "    cv_params=CV_PARAMS,\n",
        "    sweeps=SWEEPS,\n",
        "    batch_size=32,\n",
        "    num_workers=DEFAULTS.get(\"num_workers\", 4),\n",
        "    device=DEFAULTS.get(\"device\", \"cuda\"),\n",
        "    inner_agg=\"mean\",\n",
        "    outer_agg=\"mean\",\n",
        ")\n",
        "\n",
        "results\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PyTorch 2.7.1 (cu118)",
      "language": "python",
      "name": "pt27cu118"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
